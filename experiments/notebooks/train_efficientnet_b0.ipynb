{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e234458",
   "metadata": {},
   "source": [
    "# Лабораторная работа: Fine-tuning EfficientNet-B0\n",
    "\n",
    "**Цель работы:** обучить модель EfficientNet-B0 для классификации изображений трёх классов: `mouse`, `keyboard`, `soundcard`.\n",
    "\n",
    "В ноутбуке показаны: подготовка данных, обучение, валидация, сохранение модели и экспорт в ONNX.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ab4cd39",
   "metadata": {},
   "source": [
    "## 1. Импорт библиотек и настройка окружения\n",
    "\n",
    "Запустите эту ячейку для импорта необходимых библиотек."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b28b872f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Ячейка: импорт библиотек\n",
    "import os, sys, random, json, time\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import timm\n",
    "\n",
    "# Для датасета и аугментаций - используем torchvision и albumentations\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms as T\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"timm:\", timm.__version__)\n",
    "print(\"albumentations:\", A.__version__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4caf652e",
   "metadata": {},
   "source": [
    "## 2. Параметры и пути\n",
    "\n",
    "Редактируйте параметры по необходимости. Данные должны лежать в `data/raw/train` и `data/raw/val`. Модель будет сохранена в `models/best_efficientnet_b0.pth`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "456d920b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Настройки\n",
    "ROOT = Path(\".\").resolve()\n",
    "DATA_DIR = ROOT / \"data\" / \"raw\"\n",
    "TRAIN_DIR = DATA_DIR / \"train\"\n",
    "VAL_DIR = DATA_DIR / \"val\"\n",
    "OUT_DIR = ROOT / \"models\"\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Гиперпараметры\n",
    "IMAGE_SIZE = 224\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 12\n",
    "LR = 1e-4\n",
    "WEIGHT_DECAY = 1e-4\n",
    "MODEL_NAME = \"efficientnet_b0\"\n",
    "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "SEED = 42\n",
    "\n",
    "print(\"DATA_DIR:\", DATA_DIR)\n",
    "print(\"DEVICE:\", DEVICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e8dbd02",
   "metadata": {},
   "source": [
    "## 3. Для воспроизводимости и вспомогательные функции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f17f450",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Воспроизводимость\n",
    "def set_seed(seed:int=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(SEED)\n",
    "\n",
    "# Визуализация примеров\n",
    "def show_images_grid(paths, ncols=4, figsize=(12,8)):\n",
    "    n = len(paths)\n",
    "    nrows = (n + ncols - 1) // ncols\n",
    "    plt.figure(figsize=figsize)\n",
    "    for i, p in enumerate(paths):\n",
    "        ax = plt.subplot(nrows, ncols, i+1)\n",
    "        img = plt.imread(p)\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5272770b",
   "metadata": {},
   "source": [
    "## 4. Загрузка и анализ данных\n",
    "\n",
    "Проверим структуру директорий и классы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69a4923",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Списки классов и примеры\n",
    "assert TRAIN_DIR.exists(), f\"Не найдена папка {TRAIN_DIR}\"\n",
    "assert VAL_DIR.exists(), f\"Не найдена папка {VAL_DIR}\"\n",
    "\n",
    "train_classes = sorted([d.name for d in TRAIN_DIR.iterdir() if d.is_dir()])\n",
    "val_classes = sorted([d.name for d in VAL_DIR.iterdir() if d.is_dir()])\n",
    "print(\"Train classes:\", train_classes)\n",
    "print(\"Val classes:\", val_classes)\n",
    "\n",
    "# примеры файлов\n",
    "sample_paths = []\n",
    "for cls in train_classes[:3]:\n",
    "    p = TRAIN_DIR / cls\n",
    "    files = list(p.glob(\"*.jpg\")) + list(p.glob(\"*.jpeg\")) + list(p.glob(\"*.png\"))\n",
    "    sample_paths.extend(files[:4])\n",
    "\n",
    "sample_paths = sample_paths[:12]\n",
    "len(sample_paths), sample_paths[:3]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fc17ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Покажем примеры (если есть)\n",
    "if len(sample_paths)>0:\n",
    "    show_images_grid(sample_paths, ncols=4)\n",
    "else:\n",
    "    print(\"Нет изображений для предпросмотра в тренировочной папке.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c30f4a",
   "metadata": {},
   "source": [
    "## 5. Трансформации и DataLoader\n",
    "\n",
    "Для тренировочного набора используем аугментации, для валидации — простое изменение размера и нормализацию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2a9f842",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Albumentations -> torchvision compatible wrapper\n",
    "def get_albumentations_train(image_size=IMAGE_SIZE):\n",
    "    return A.Compose([\n",
    "        A.RandomResizedCrop(image_size, image_size, scale=(0.8,1.0)),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.Rotate(limit=20, p=0.3),\n",
    "        A.ColorJitter(0.2,0.2,0.2, p=0.3),\n",
    "        A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "def get_albumentations_val(image_size=IMAGE_SIZE):\n",
    "    return A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.Normalize(mean=(0.485,0.456,0.406), std=(0.229,0.224,0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "# Wrapper dataset to apply albumentations on PIL images from ImageFolder\n",
    "from torchvision.datasets import ImageFolder\n",
    "class AlbumentationsImageFolder(ImageFolder):\n",
    "    def __init__(self, root, transform=None, alb_transform=None):\n",
    "        super().__init__(root, transform=transform)\n",
    "        self.alb_transform = alb_transform\n",
    "    def __getitem__(self, index):\n",
    "        path, target = self.samples[index]\n",
    "        # PIL open\n",
    "        img = plt.imread(path)\n",
    "        # If image has alpha channel, drop it\n",
    "        if img.ndim == 2:\n",
    "            img = np.stack([img]*3, axis=-1)\n",
    "        if img.shape[2] == 4:\n",
    "            img = img[..., :3]\n",
    "        if self.alb_transform:\n",
    "            augmented = self.alb_transform(image=img)\n",
    "            img = augmented['image']\n",
    "        return img, target\n",
    "\n",
    "train_alb = AlbumentationsImageFolder(str(TRAIN_DIR), alb_transform=get_albumentations_train())\n",
    "val_alb = AlbumentationsImageFolder(str(VAL_DIR), alb_transform=get_albumentations_val())\n",
    "\n",
    "print(\"Samples train:\", len(train_alb), \" samples val:\", len(val_alb))\n",
    "class_names = train_alb.classes\n",
    "print(\"Class names:\", class_names)\n",
    "\n",
    "train_loader = DataLoader(train_alb, batch_size=BATCH_SIZE, shuffle=True, num_workers=2)\n",
    "val_loader = DataLoader(val_alb, batch_size=BATCH_SIZE, shuffle=False, num_workers=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cef445",
   "metadata": {},
   "source": [
    "## 6. Создание модели EfficientNet-B0\n",
    "\n",
    "Используем `timm` для загрузки предобученной архитетуры и подгонки под количество классов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2d8edb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=len(class_names))\n",
    "model.to(DEVICE)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04773510",
   "metadata": {},
   "source": [
    "## 7. Обучение модели\n",
    "\n",
    "Цикл обучения с сохранением лучшей модели по валидационной точности."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19fd659",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "best_val_acc = 0.0\n",
    "history = {'train_loss':[], 'val_loss':[], 'val_acc':[]}\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    count = 0\n",
    "    for imgs, labels in train_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        labels = labels.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(imgs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * imgs.size(0)\n",
    "        count += imgs.size(0)\n",
    "    train_loss = running_loss / count\n",
    "    history['train_loss'].append(train_loss)\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    v_loss = 0.0\n",
    "    v_count = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(imgs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            v_loss += loss.item() * imgs.size(0)\n",
    "            v_count += imgs.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "    val_loss = v_loss / v_count\n",
    "    val_acc = correct / total\n",
    "    history['val_loss'].append(val_loss)\n",
    "    history['val_acc'].append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch}/{EPOCHS} - train_loss: {train_loss:.4f}  val_loss: {val_loss:.4f}  val_acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Save best\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        torch.save(model.state_dict(), OUT_DIR / \"best_efficientnet_b0.pth\")\n",
    "        print(\"Saved best model.\")\n",
    "    scheduler.step()\n",
    "\n",
    "# Сохраним историю\n",
    "with open(OUT_DIR / \"history_efficientnet_b0.json\", \"w\") as f:\n",
    "    json.dump(history, f, indent=2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd071470",
   "metadata": {},
   "source": [
    "## 8. Построение графиков и матрицы ошибок"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a15ccbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Графики loss/acc\n",
    "hist = history\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(hist['train_loss'], label='train_loss')\n",
    "plt.plot(hist['val_loss'], label='val_loss')\n",
    "plt.legend(); plt.title('Loss')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(hist['val_acc'], label='val_acc')\n",
    "plt.legend(); plt.title('Val Accuracy')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c248ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Confusion matrix and classification report\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# Load best model for evaluation\n",
    "best = OUT_DIR / \"best_efficientnet_b0.pth\"\n",
    "if best.exists():\n",
    "    model.load_state_dict(torch.load(best, map_location=DEVICE))\n",
    "    model.to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for imgs, labels in val_loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            labels = labels.to(DEVICE)\n",
    "            outputs = model(imgs)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy().tolist())\n",
    "            all_labels.extend(labels.cpu().numpy().tolist())\n",
    "\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix')\n",
    "    plt.show()\n",
    "\n",
    "    print(classification_report(all_labels, all_preds, target_names=class_names))\n",
    "else:\n",
    "    print(\"Best model not found - no evaluation.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93340ae9",
   "metadata": {},
   "source": [
    "## 9. Экспорт в ONNX и проверка через ONNX Runtime\n",
    "\n",
    "Экспортируем лучшую модель в формат ONNX и проверим с помощью onnxruntime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cd388a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import onnx\n",
    "import torch.onnx\n",
    "import onnxruntime as ort\n",
    "\n",
    "best = OUT_DIR / \"best_efficientnet_b0.pth\"\n",
    "onnx_out = OUT_DIR / \"efficientnet_b0.onnx\"\n",
    "\n",
    "if best.exists():\n",
    "    # создаём модель и dummy input\n",
    "    model_cpu = timm.create_model(MODEL_NAME, pretrained=False, num_classes=len(class_names))\n",
    "    model_cpu.load_state_dict(torch.load(best, map_location='cpu'))\n",
    "    model_cpu.eval()\n",
    "\n",
    "    dummy = torch.randn(1,3,IMAGE_SIZE,IMAGE_SIZE, device='cpu')\n",
    "    torch.onnx.export(model_cpu, dummy, str(onnx_out), opset_version=18,\n",
    "                      input_names=['input'], output_names=['output'], dynamic_axes={'input':{0:'batch_size'}, 'output':{0:'batch_size'}})\n",
    "    print(\"ONNX exported to\", onnx_out)\n",
    "\n",
    "    # Проверка\n",
    "    onnx_model = onnx.load(str(onnx_out))\n",
    "    onnx.checker.check_model(onnx_model)\n",
    "    print(\"ONNX model checked.\")\n",
    "\n",
    "    # Быстрая инференс-проверка через onnxruntime\n",
    "    ort_sess = ort.InferenceSession(str(onnx_out), providers=['CPUExecutionProvider'])\n",
    "    import numpy as np\n",
    "    x = np.random.randn(1,3,IMAGE_SIZE,IMAGE_SIZE).astype(np.float32)\n",
    "    out = ort_sess.run(None, {'input': x})\n",
    "    print(\"ONNX runtime output shape:\", np.array(out[0]).shape)\n",
    "else:\n",
    "    print(\"Best model weights not found, skipping ONNX export.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9a949f",
   "metadata": {},
   "source": [
    "## 10. Выводы\n",
    "\n",
    "Кратко опишите результаты: точности, наблюдения (что модель путает), предложения по улучшению (больше данных, дополнительные аугментации, балансировка классов, изменение learning rate и/или полная разморозка backbone и дообучение)."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
