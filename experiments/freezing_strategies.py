# experiments/freezing_strategies.py
"""
–ê–Ω–∞–ª–∏–∑ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏—è/—Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏—è –≤–µ—Å–æ–≤
"""

import os
import json
import matplotlib.pyplot as plt
from datetime import datetime
from typing import Dict, List, Tuple
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import timm

from datasets import ImageFolderSimple, get_transforms
from utils import set_seed


class FreezingStrategyAnalyzer:
    """–ê–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏—è"""
    
    def __init__(self, data_dir: str, train_subdir: str, val_subdir: str, 
                 out_dir: str, logs_dir: str, device: str = "cuda"):
        self.data_dir = data_dir
        self.train_subdir = train_subdir
        self.val_subdir = val_subdir
        self.out_dir = out_dir
        self.logs_dir = logs_dir
        self.device = device
        
        os.makedirs(out_dir, exist_ok=True)
        os.makedirs(logs_dir, exist_ok=True)
    
    def freeze_backbone(self, model, freeze: bool = True):
        """–ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏–µ/—Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏–µ backbone –º–æ–¥–µ–ª–∏"""
        for name, param in model.named_parameters():
            if 'head' in name or 'classifier' in name or 'fc' in name:
                param.requires_grad = True  # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≤—Å–µ–≥–¥–∞ –æ–±—É—á–∞–µ—Ç—Å—è
            else:
                param.requires_grad = not freeze
    
    def count_parameters(self, model):
        """–ü–æ–¥—Å—á–µ—Ç –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        total = sum(p.numel() for p in model.parameters())
        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        return total, trainable
    
    def train_with_strategy(self, model_name: str, strategy: str, 
                           train_loader: DataLoader, val_loader: DataLoader,
                           num_classes: int, epochs: int = 12) -> Dict:
        """–û–±—É—á–µ–Ω–∏–µ —Å –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–Ω–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–µ–π –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏—è"""
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model = timm.create_model(model_name, pretrained=True, num_classes=num_classes)
        model.to(self.device)
        
        # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–æ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏—è
        total_params, initial_trainable = self.count_parameters(model)
        
        # –ü—Ä–∏–º–µ–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏—è
        if strategy == "no_freeze":
            # –ë–µ–∑ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏—è - –æ–±—É—á–∞–µ–º –≤—Å–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã
            pass
        elif strategy == "freeze_always":
            # –í—Å–µ–≥–¥–∞ –∑–∞–º–æ—Ä–æ–∂–µ–Ω backbone
            self.freeze_backbone(model, freeze=True)
        elif strategy == "freeze_early":
            # –ó–∞–º–æ—Ä–æ–∂–µ–Ω –ø–µ—Ä–≤—ã–µ 5 —ç–ø–æ—Ö, –ø–æ—Ç–æ–º —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω
            self.freeze_backbone(model, freeze=True)
        elif strategy == "freeze_mid":
            # –ó–∞–º–æ—Ä–æ–∂–µ–Ω –ø–µ—Ä–≤—ã–µ 3 —ç–ø–æ—Ö–∏, –ø–æ—Ç–æ–º —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω
            self.freeze_backbone(model, freeze=True)
        elif strategy == "freeze_late":
            # –ó–∞–º–æ—Ä–æ–∂–µ–Ω –ø–µ—Ä–≤—ã–µ 8 —ç–ø–æ—Ö, –ø–æ—Ç–æ–º —Ä–∞–∑–º–æ—Ä–æ–∂–µ–Ω
            self.freeze_backbone(model, freeze=True)
        
        # –ü–æ–¥—Å—á–µ—Ç –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –ø–æ—Å–ª–µ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏—è
        _, frozen_trainable = self.count_parameters(model)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        optimizer = torch.optim.AdamW(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=1e-3, weight_decay=1e-4
        )
        
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)
        criterion = nn.CrossEntropyLoss()
        
        # –ò—Å—Ç–æ—Ä–∏—è –æ–±—É—á–µ–Ω–∏—è
        history = {
            'train_loss': [],
            'val_loss': [],
            'val_acc': [],
            'lr': [],
            'trainable_params': []
        }
        
        best_val_acc = 0.0
        best_epoch = 0
        
        print(f"\n–°—Ç—Ä–∞—Ç–µ–≥–∏—è: {strategy}")
        print(f"–í—Å–µ–≥–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {total_params:,}")
        print(f"–û–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤: {frozen_trainable:,}")
        
        # –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
        for epoch in range(1, epochs + 1):
            # –û–±—É—á–µ–Ω–∏–µ
            model.train()
            train_loss = 0.0
            train_correct = 0
            train_total = 0
            
            for imgs, labels in train_loader:
                imgs, labels = imgs.to(self.device), labels.to(self.device)
                optimizer.zero_grad()
                outputs = model(imgs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item() * imgs.size(0)
                preds = outputs.argmax(dim=1)
                train_correct += (preds == labels).sum().item()
                train_total += labels.size(0)
            
            train_loss /= train_total
            train_acc = train_correct / train_total
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            model.eval()
            val_loss = 0.0
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for imgs, labels in val_loader:
                    imgs, labels = imgs.to(self.device), labels.to(self.device)
                    outputs = model(imgs)
                    loss = criterion(outputs, labels)
                    
                    val_loss += loss.item() * imgs.size(0)
                    preds = outputs.argmax(dim=1)
                    val_correct += (preds == labels).sum().item()
                    val_total += labels.size(0)
            
            val_loss /= val_total
            val_acc = val_correct / val_total
            
            # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏
            history['train_loss'].append(train_loss)
            history['val_loss'].append(val_loss)
            history['val_acc'].append(val_acc)
            history['lr'].append(optimizer.param_groups[0]['lr'])
            history['trainable_params'].append(self.count_parameters(model)[1])
            
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                best_epoch = epoch
            
            print(f"Epoch {epoch:2d}: Train Loss={train_loss:.4f} | "
                  f"Val Loss={val_loss:.4f} | Val Acc={val_acc:.4f}")
            
            # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏–µ –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏
            if strategy == "freeze_early" and epoch == 5:
                self.freeze_backbone(model, freeze=False)
                for g in optimizer.param_groups:
                    g['lr'] *= 0.1
                print(f"üîì –†–∞–∑–º–æ—Ä–æ–∂–µ–Ω backbone –Ω–∞ —ç–ø–æ—Ö–µ {epoch}")
            elif strategy == "freeze_mid" and epoch == 3:
                self.freeze_backbone(model, freeze=False)
                for g in optimizer.param_groups:
                    g['lr'] *= 0.1
                print(f"üîì –†–∞–∑–º–æ—Ä–æ–∂–µ–Ω backbone –Ω–∞ —ç–ø–æ—Ö–µ {epoch}")
            elif strategy == "freeze_late" and epoch == 8:
                self.freeze_backbone(model, freeze=False)
                for g in optimizer.param_groups:
                    g['lr'] *= 0.1
                print(f"üîì –†–∞–∑–º–æ—Ä–æ–∂–µ–Ω backbone –Ω–∞ —ç–ø–æ—Ö–µ {epoch}")
            
            scheduler.step()
        
        return {
            'strategy': strategy,
            'best_val_acc': best_val_acc,
            'best_epoch': best_epoch,
            'total_params': total_params,
            'initial_trainable': initial_trainable,
            'final_trainable': self.count_parameters(model)[1],
            'history': history
        }
    
    def compare_strategies(self, model_name: str, strategies: List[str],
                          train_loader: DataLoader, val_loader: DataLoader,
                          num_classes: int, epochs: int = 12) -> Dict:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏—è"""
        
        results = {}
        
        print(f"\n{'='*60}")
        print(f"–°–†–ê–í–ù–ï–ù–ò–ï –°–¢–†–ê–¢–ï–ì–ò–ô –ó–ê–ú–û–†–ê–ñ–ò–í–ê–ù–ò–Ø –î–õ–Ø {model_name.upper()}")
        print(f"{'='*60}")
        
        for strategy in strategies:
            print(f"\n{'='*40}")
            print(f"–¢–µ—Å—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏—é: {strategy}")
            print(f"{'='*40}")
            
            result = self.train_with_strategy(
                model_name, strategy, train_loader, val_loader, num_classes, epochs
            )
            results[strategy] = result
        
        return results
    
    def visualize_strategies(self, results: Dict, model_name: str):
        """–í–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å—Ç—Ä–∞—Ç–µ–≥–∏–π"""
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        
        # –ì—Ä–∞—Ñ–∏–∫ —Ç–æ—á–Ω–æ—Å—Ç–∏
        for strategy, result in results.items():
            axes[0, 0].plot(result['history']['val_acc'], 
                           label=f"{strategy} (best: {result['best_val_acc']:.4f})", 
                           marker='o')
        axes[0, 0].set_title('–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º')
        axes[0, 0].set_xlabel('–≠–ø–æ—Ö–∞')
        axes[0, 0].set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
        axes[0, 0].legend()
        axes[0, 0].grid(True)
        
        # –ì—Ä–∞—Ñ–∏–∫ loss
        for strategy, result in results.items():
            axes[0, 1].plot(result['history']['train_loss'], 
                           label=f"{strategy} (train)", linestyle='--')
            axes[0, 1].plot(result['history']['val_loss'], 
                           label=f"{strategy} (val)", linestyle='-')
        axes[0, 1].set_title('Loss –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º')
        axes[0, 1].set_xlabel('–≠–ø–æ—Ö–∞')
        axes[0, 1].set_ylabel('Loss')
        axes[0, 1].legend()
        axes[0, 1].grid(True)
        
        # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        strategies = list(results.keys())
        best_accs = [results[s]['best_val_acc'] for s in strategies]
        axes[1, 0].bar(strategies, best_accs)
        axes[1, 0].set_title('–õ—É—á—à–∞—è –≤–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å')
        axes[1, 0].set_ylabel('–¢–æ—á–Ω–æ—Å—Ç—å')
        axes[1, 0].tick_params(axis='x', rotation=45)
        
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        initial_trainable = [results[s]['initial_trainable'] for s in strategies]
        final_trainable = [results[s]['final_trainable'] for s in strategies]
        
        x = np.arange(len(strategies))
        width = 0.35
        
        axes[1, 1].bar(x - width/2, initial_trainable, width, label='–ù–∞—á–∞–ª—å–Ω—ã–µ –æ–±—É—á–∞–µ–º—ã–µ')
        axes[1, 1].bar(x + width/2, final_trainable, width, label='–§–∏–Ω–∞–ª—å–Ω—ã–µ –æ–±—É—á–∞–µ–º—ã–µ')
        axes[1, 1].set_title('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ–±—É—á–∞–µ–º—ã—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤')
        axes[1, 1].set_ylabel('–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤')
        axes[1, 1].set_xticks(x)
        axes[1, 1].set_xticklabels(strategies, rotation=45)
        axes[1, 1].legend()
        
        plt.tight_layout()
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        plot_path = os.path.join(self.logs_dir, f"freezing_strategies_{model_name}_{timestamp}.png")
        plt.savefig(plot_path, dpi=300, bbox_inches='tight')
        plt.show()
        
        print(f"–ì—Ä–∞—Ñ–∏–∫ —Å–æ—Ö—Ä–∞–Ω–µ–Ω: {plot_path}")
    
    def save_results(self, results: Dict, model_name: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏
        sorted_results = sorted(results.items(), key=lambda x: x[1]['best_val_acc'], reverse=True)
        
        results_path = os.path.join(self.logs_dir, f"freezing_strategies_{model_name}_{timestamp}.json")
        with open(results_path, 'w') as f:
            json.dump(dict(sorted_results), f, indent=2)
        
        print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã: {results_path}")
        
        # –í—ã–≤–æ–¥ –ª—É—á—à–∏—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π
        print(f"\nüèÜ –†–ï–ô–¢–ò–ù–ì –°–¢–†–ê–¢–ï–ì–ò–ô –î–õ–Ø {model_name.upper()}:")
        for i, (strategy, result) in enumerate(sorted_results):
            print(f"{i+1}. {strategy:15s}: {result['best_val_acc']:.4f} "
                  f"(—ç–ø–æ—Ö–∞ {result['best_epoch']})")


def run_freezing_analysis():
    """–ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏—è"""
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    data_dir = "data/raw"
    train_subdir = "train"
    val_subdir = "val"
    out_dir = "models"
    logs_dir = "experiments/logs"
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    set_seed(42)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train_t, val_t = get_transforms(224)
    train_ds = ImageFolderSimple(os.path.join(data_dir, train_subdir), transform=train_t)
    val_ds = ImageFolderSimple(os.path.join(data_dir, val_subdir), 
                               transform=val_t, classes=list(train_ds.class_to_idx.keys()))
    
    num_classes = len(train_ds.class_to_idx)
    class_names = list(train_ds.class_to_idx.keys())
    
    print(f"–ö–ª–∞—Å—Å—ã: {class_names}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {num_classes}")
    
    # DataLoader'—ã
    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
    analyzer = FreezingStrategyAnalyzer(data_dir, train_subdir, val_subdir, out_dir, logs_dir, device)
    
    # –°—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    strategies = [
        "no_freeze",      # –ë–µ–∑ –∑–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏—è
        "freeze_always",  # –í—Å–µ–≥–¥–∞ –∑–∞–º–æ—Ä–æ–∂–µ–Ω
        "freeze_early",   # –†–∞–∑–º–æ—Ä–æ–∑–∫–∞ –Ω–∞ 5 —ç–ø–æ—Ö–µ
        "freeze_mid",     # –†–∞–∑–º–æ—Ä–æ–∑–∫–∞ –Ω–∞ 3 —ç–ø–æ—Ö–µ
        "freeze_late"     # –†–∞–∑–º–æ—Ä–æ–∑–∫–∞ –Ω–∞ 8 —ç–ø–æ—Ö–µ
    ]
    
    # –ú–æ–¥–µ–ª–∏ –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    models = ['resnet18', 'efficientnet_b0']
    
    all_results = {}
    
    for model_name in models:
        print(f"\n{'='*80}")
        print(f"–ê–ù–ê–õ–ò–ó –°–¢–†–ê–¢–ï–ì–ò–ô –ó–ê–ú–û–†–ê–ñ–ò–í–ê–ù–ò–Ø –î–õ–Ø {model_name.upper()}")
        print(f"{'='*80}")
        
        results = analyzer.compare_strategies(
            model_name, strategies, train_loader, val_loader, num_classes, epochs=10
        )
        
        analyzer.visualize_strategies(results, model_name)
        analyzer.save_results(results, model_name)
        
        all_results[model_name] = results
    
    # –û–±—â–∏–π –∞–Ω–∞–ª–∏–∑
    print(f"\n{'='*80}")
    print("–û–ë–©–ò–ô –ê–ù–ê–õ–ò–ó –°–¢–†–ê–¢–ï–ì–ò–ô –ó–ê–ú–û–†–ê–ñ–ò–í–ê–ù–ò–Ø")
    print(f"{'='*80}")
    
    for model_name, results in all_results.items():
        print(f"\n{model_name.upper()}:")
        sorted_results = sorted(results.items(), key=lambda x: x[1]['best_val_acc'], reverse=True)
        for strategy, result in sorted_results:
            print(f"  {strategy:15s}: {result['best_val_acc']:.4f}")
    
    return all_results


if __name__ == "__main__":
    results = run_freezing_analysis()
