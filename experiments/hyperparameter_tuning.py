# experiments/hyperparameter_tuning.py
"""
–†–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–π –ø–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –º–æ–¥–µ–ª–µ–π ResNet18 –∏ EfficientNet-B0
"""

import os
import json
import itertools
from datetime import datetime
from typing import Dict, List, Tuple, Any
import numpy as np
import torch
import torch.nn as nn
from torch.utils.data import DataLoader
import timm
from sklearn.metrics import accuracy_score

from datasets import ImageFolderSimple, get_transforms
from utils import set_seed


class HyperparameterTuner:
    """–ö–ª–∞—Å—Å –¥–ª—è –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
    
    def __init__(self, data_dir: str, train_subdir: str, val_subdir: str, 
                 out_dir: str, logs_dir: str, device: str = "cuda"):
        self.data_dir = data_dir
        self.train_subdir = train_subdir
        self.val_subdir = val_subdir
        self.out_dir = out_dir
        self.logs_dir = logs_dir
        self.device = device
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–π
        os.makedirs(out_dir, exist_ok=True)
        os.makedirs(logs_dir, exist_ok=True)
    
    def freeze_backbone(self, model, freeze: bool = True):
        """–ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏–µ/—Ä–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏–µ backbone –º–æ–¥–µ–ª–∏"""
        for name, param in model.named_parameters():
            if 'head' in name or 'classifier' in name or 'fc' in name:
                param.requires_grad = True  # –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä –≤—Å–µ–≥–¥–∞ –æ–±—É—á–∞–µ—Ç—Å—è
            else:
                param.requires_grad = not freeze
    
    def train_single_config(self, model_name: str, config: Dict, 
                           train_loader: DataLoader, val_loader: DataLoader,
                           epochs: int = 8) -> Dict:
        """–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –æ–¥–Ω–æ–π –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        
        # –°–æ–∑–¥–∞–Ω–∏–µ –º–æ–¥–µ–ª–∏
        model = timm.create_model(
            model_name, 
            pretrained=True, 
            num_classes=config['num_classes']
        )
        model.to(self.device)
        
        # –ó–∞–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏–µ backbone
        if config['freeze_backbone']:
            self.freeze_backbone(model, freeze=True)
        
        # –û–ø—Ç–∏–º–∏–∑–∞—Ç–æ—Ä
        if config['optimizer'] == 'AdamW':
            optimizer = torch.optim.AdamW(
                filter(lambda p: p.requires_grad, model.parameters()),
                lr=config['lr'],
                weight_decay=config['weight_decay']
            )
        elif config['optimizer'] == 'SGD':
            optimizer = torch.optim.SGD(
                filter(lambda p: p.requires_grad, model.parameters()),
                lr=config['lr'],
                weight_decay=config['weight_decay'],
                momentum=0.9
            )
        
        # –ü–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫
        if config['scheduler'] == 'StepLR':
            scheduler = torch.optim.lr_scheduler.StepLR(
                optimizer, step_size=config['step_size'], gamma=config['gamma']
            )
        elif config['scheduler'] == 'CosineAnnealingLR':
            scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
                optimizer, T_max=epochs
            )
        elif config['scheduler'] == 'ReduceLROnPlateau':
            scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(
                optimizer, mode='max', patience=2, factor=0.5
            )
        
        criterion = nn.CrossEntropyLoss()
        
        best_val_acc = 0.0
        history = {'val_acc': []}
        
        # –¶–∏–∫–ª –æ–±—É—á–µ–Ω–∏—è
        for epoch in range(epochs):
            # –û–±—É—á–µ–Ω–∏–µ
            model.train()
            for imgs, labels in train_loader:
                imgs, labels = imgs.to(self.device), labels.to(self.device)
                optimizer.zero_grad()
                outputs = model(imgs)
                loss = criterion(outputs, labels)
                loss.backward()
                optimizer.step()
            
            # –í–∞–ª–∏–¥–∞—Ü–∏—è
            model.eval()
            val_correct = 0
            val_total = 0
            
            with torch.no_grad():
                for imgs, labels in val_loader:
                    imgs, labels = imgs.to(self.device), labels.to(self.device)
                    outputs = model(imgs)
                    preds = outputs.argmax(dim=1)
                    val_correct += (preds == labels).sum().item()
                    val_total += labels.size(0)
            
            val_acc = val_correct / val_total
            history['val_acc'].append(val_acc)
            
            if val_acc > best_val_acc:
                best_val_acc = val_acc
            
            # –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞
            if config['scheduler'] == 'ReduceLROnPlateau':
                scheduler.step(val_acc)
            else:
                scheduler.step()
            
            # –†–∞–∑–º–æ—Ä–∞–∂–∏–≤–∞–Ω–∏–µ backbone
            if (config['freeze_backbone'] and 
                epoch == config.get('unfreeze_at_epoch', 3)):
                self.freeze_backbone(model, freeze=False)
                for g in optimizer.param_groups:
                    g['lr'] *= 0.1
        
        return {
            'best_val_acc': best_val_acc,
            'history': history,
            'config': config
        }
    
    def grid_search(self, model_name: str, param_grid: Dict, 
                   train_loader: DataLoader, val_loader: DataLoader,
                   num_classes: int, epochs: int = 8) -> List[Dict]:
        """–ü–æ–∏—Å–∫ –ø–æ —Å–µ—Ç–∫–µ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤"""
        
        # –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –≤—Å–µ—Ö –∫–æ–º–±–∏–Ω–∞—Ü–∏–π –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
        param_names = list(param_grid.keys())
        param_values = list(param_grid.values())
        param_combinations = list(itertools.product(*param_values))
        
        results = []
        total_combinations = len(param_combinations)
        
        print(f"–ù–∞—á–∏–Ω–∞–µ–º –ø–æ–∏—Å–∫ –ø–æ —Å–µ—Ç–∫–µ –¥–ª—è {model_name}")
        print(f"–í—Å–µ–≥–æ –∫–æ–º–±–∏–Ω–∞—Ü–∏–π: {total_combinations}")
        
        for i, combination in enumerate(param_combinations):
            config = dict(zip(param_names, combination))
            config['num_classes'] = num_classes
            
            print(f"\n[{i+1}/{total_combinations}] –¢–µ—Å—Ç–∏—Ä—É–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é:")
            for key, value in config.items():
                if key != 'num_classes':
                    print(f"  {key}: {value}")
            
            try:
                result = self.train_single_config(
                    model_name, config, train_loader, val_loader, epochs
                )
                result['model_name'] = model_name
                results.append(result)
                
                print(f"  –†–µ–∑—É–ª—å—Ç–∞—Ç: {result['best_val_acc']:.4f}")
                
            except Exception as e:
                print(f"  –û—à–∏–±–∫–∞: {e}")
                continue
        
        return results
    
    def save_results(self, results: List[Dict], model_name: str):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –ø–æ–∏—Å–∫–∞"""
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # –°–æ—Ä—Ç–∏—Ä–æ–≤–∫–∞ –ø–æ —Ç–æ—á–Ω–æ—Å—Ç–∏
        results.sort(key=lambda x: x['best_val_acc'], reverse=True)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –≤ JSON
        results_path = os.path.join(self.logs_dir, f"hyperparameter_search_{model_name}_{timestamp}.json")
        with open(results_path, 'w') as f:
            json.dump(results, f, indent=2)
        
        # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        best_results = results[:5]  # –¢–æ–ø-5
        best_path = os.path.join(self.logs_dir, f"best_configs_{model_name}_{timestamp}.json")
        with open(best_path, 'w') as f:
            json.dump(best_results, f, indent=2)
        
        print(f"\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã:")
        print(f"  –í—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã: {results_path}")
        print(f"  –õ—É—á—à–∏–µ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {best_path}")
        
        return results


def run_hyperparameter_tuning():
    """–ó–∞–ø—É—Å–∫ –ø–æ–¥–±–æ—Ä–∞ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –æ–±–µ–∏—Ö –º–æ–¥–µ–ª–µ–π"""
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∏
    data_dir = "data/raw"
    train_subdir = "train"
    val_subdir = "val"
    out_dir = "models"
    logs_dir = "experiments/logs"
    device = "cuda" if torch.cuda.is_available() else "cpu"
    
    set_seed(42)
    
    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö
    train_t, val_t = get_transforms(224)
    train_ds = ImageFolderSimple(os.path.join(data_dir, train_subdir), transform=train_t)
    val_ds = ImageFolderSimple(os.path.join(data_dir, val_subdir), 
                               transform=val_t, classes=list(train_ds.class_to_idx.keys()))
    
    num_classes = len(train_ds.class_to_idx)
    class_names = list(train_ds.class_to_idx.keys())
    
    print(f"–ö–ª–∞—Å—Å—ã: {class_names}")
    print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∫–ª–∞—Å—Å–æ–≤: {num_classes}")
    print(f"–¢—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {len(train_ds)}")
    print(f"–í–∞–ª–∏–¥–∞—Ü–∏–æ–Ω–Ω—ã—Ö –æ–±—Ä–∞–∑—Ü–æ–≤: {len(val_ds)}")
    
    # DataLoader'—ã
    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True, num_workers=2)
    val_loader = DataLoader(val_ds, batch_size=16, shuffle=False, num_workers=2)
    
    # –°–æ–∑–¥–∞–Ω–∏–µ —Ç—é–Ω–µ—Ä–∞
    tuner = HyperparameterTuner(data_dir, train_subdir, val_subdir, out_dir, logs_dir, device)
    
    # –°–µ—Ç–∫–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    param_grids = {
        'resnet18': {
            'lr': [1e-3, 5e-4, 1e-4],
            'weight_decay': [1e-4, 1e-3],
            'optimizer': ['AdamW', 'SGD'],
            'scheduler': ['StepLR', 'CosineAnnealingLR'],
            'freeze_backbone': [True, False],
            'step_size': [3, 5],
            'gamma': [0.1, 0.5]
        },
        'efficientnet_b0': {
            'lr': [1e-3, 5e-4, 1e-4],
            'weight_decay': [1e-4, 1e-3],
            'optimizer': ['AdamW', 'SGD'],
            'scheduler': ['StepLR', 'CosineAnnealingLR'],
            'freeze_backbone': [True, False],
            'step_size': [3, 5],
            'gamma': [0.1, 0.5]
        }
    }
    
    all_results = {}
    
    # –ü–æ–¥–±–æ—Ä –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤ –¥–ª—è –∫–∞–∂–¥–æ–π –º–æ–¥–µ–ª–∏
    for model_name, param_grid in param_grids.items():
        print(f"\n{'='*60}")
        print(f"–ü–û–î–ë–û–† –ì–ò–ü–ï–†–ü–ê–†–ê–ú–ï–¢–†–û–í –î–õ–Ø {model_name.upper()}")
        print(f"{'='*60}")
        
        results = tuner.grid_search(
            model_name, param_grid, train_loader, val_loader, num_classes, epochs=6
        )
        
        tuner.save_results(results, model_name)
        all_results[model_name] = results
        
        # –í—ã–≤–æ–¥ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
        print(f"\nüèÜ –¢–û–ü-5 –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–ô –î–õ–Ø {model_name.upper()}:")
        for i, result in enumerate(results[:5]):
            print(f"\n{i+1}. –¢–æ—á–Ω–æ—Å—Ç—å: {result['best_val_acc']:.4f}")
            config = result['config']
            for key, value in config.items():
                if key != 'num_classes':
                    print(f"   {key}: {value}")
    
    # –°—Ä–∞–≤–Ω–µ–Ω–∏–µ –ª—É—á—à–∏—Ö —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print(f"\n{'='*60}")
    print("–°–†–ê–í–ù–ï–ù–ò–ï –õ–£–ß–®–ò–• –†–ï–ó–£–õ–¨–¢–ê–¢–û–í")
    print(f"{'='*60}")
    
    for model_name, results in all_results.items():
        if results:
            best_result = results[0]
            print(f"{model_name:15s}: {best_result['best_val_acc']:.4f}")
    
    return all_results


if __name__ == "__main__":
    results = run_hyperparameter_tuning()
